{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "data = pd.read_csv(\"./preprocessed_data.csv\")\n",
    "food = pd.read_csv(\"./food_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(food.columns)\n",
    "print(food.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set id for matching\n",
    "food.index = food.id\n",
    "data.index = data.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Step by Step\n",
    "\n",
    "1. PCA for dimensionality reduction\n",
    "    - decide what components to keep\n",
    "2. Use elbow method to determine how many clusters to use\n",
    "3. Kmeans clustering\n",
    "4. Evaluation of kmeans prediction sillhouette score\n",
    "5. Get three other closest points to the prediction used as our recommendations\n",
    "    - map them to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "s = StandardScaler()\n",
    "scaled_data = s.fit_transform(data)\n",
    "scaled_data\n",
    "\n",
    "# Perform PCA first for dimensionality reduction\n",
    "pca = PCA(2)\n",
    "pca_dat = pca.fit_transform(scaled_data)\n",
    "pca_result = pd.DataFrame(pca_dat, columns=[\"PC-1\", \"PC-2\"], index=data.index)\n",
    "\n",
    "# Performing K-means itterations to get inertia for determining clusters\n",
    "intertia_list = []\n",
    "for k in range(1, 9):\n",
    "    kmeans = KMeans(n_clusters=k, init=\"k-means++\")\n",
    "    kmeans.fit(pca_result)\n",
    "    intertia_list.append(kmeans.inertia_)\n",
    "\n",
    "K = 9\n",
    "plt.plot(range(1,K), intertia_list, 'purple')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that the fall off point for the elbow method is 3 thus we should proceed with an optimal value of three clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Perform optimal k means\n",
    "kmeans_m1 = KMeans(n_clusters=3, init=\"k-means++\")\n",
    "kmeans_centers = np.array(\n",
    "    kmeans_m1.fit(pca_result).cluster_centers_)\n",
    "label = kmeans_m1.fit_predict(pca_result)\n",
    "\n",
    "plt.scatter(pca_result[\"PC-1\"], \n",
    "            pca_result[\"PC-2\"],\n",
    "            c=label, cmap=\"Pastel2\")\n",
    "plt.scatter(kmeans_centers[:, 0],\n",
    "            kmeans_centers[:, 1],\n",
    "            marker=\"o\", c=\"red\")\n",
    "plt.xlabel(\"PCA_1\")\n",
    "plt.ylabel(\"PCA_2\")\n",
    "plt.title(\"kmeans_plot\")\n",
    "plt.show()\n",
    "\n",
    "food['cluster_id'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "sim_matrix = pd.DataFrame(distance_matrix(pca_result.values, pca_result.values), index=pca_result.index, columns=pca_result.index)\n",
    "sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a small input of what a user could possibly search in the system\n",
    "\n",
    "#user_input = \"greek romaine lettuce cheese\"\n",
    "#user_input = \"japanese noodles egg pork\"\n",
    "user_input = \"greek romaine lettuce feta\"\n",
    "# search variables\n",
    "def food_recommender(user_input):\n",
    "    cuisine_type = None\n",
    "    cuisine_dict = food.cuisine.unique()\n",
    "    filtered_food = pd.DataFrame()\n",
    "    food[\"count\"] = 0\n",
    "    \n",
    "    for cuisine in cuisine_dict:\n",
    "        if cuisine in user_input:\n",
    "            cuisine_type = cuisine\n",
    "    \n",
    "    for index, row in food.iterrows():\n",
    "        \n",
    "        word_relevancy = 0\n",
    "        user_split = user_input.split(\" \")\n",
    "\n",
    "        for word in user_split:\n",
    "            if word in row.ingredients:\n",
    "                word_relevancy +=1\n",
    "\n",
    "        if word_relevancy != 0:\n",
    "            row[\"count\"] = word_relevancy\n",
    "            filtered_food = filtered_food.append(row)\n",
    "            \n",
    "    if cuisine_type:\n",
    "        filtered_food = filtered_food[filtered_food[\"cuisine\"] == cuisine_type]\n",
    "    \n",
    "    filtered_food = filtered_food[filtered_food[\"count\"] == filtered_food[\"count\"].max()]\n",
    "    \n",
    "    print(\"Similar results gathered from query: \" + str(filtered_food.shape[0]))\n",
    "    \n",
    "    base_rec = random.choice(list(filtered_food[\"id\"]))\n",
    "    base_rec = filtered_food[filtered_food[\"id\"] == base_rec]\n",
    "    base_rec.reset_index(drop=True)\n",
    "    food_det = base_rec[\"id\"]\n",
    "    \n",
    "    print(\"Food identified: \" + food_det.to_string(index=False))\n",
    "    print(\"Relevant key words: \" + str(base_rec['count'].to_string(index=False)))\n",
    "    print(\"Ingredients list :\" + str(base_rec.ingredients.to_string(index=False)))\n",
    "    \n",
    "    print('\\n')\n",
    "    print('[Recommendations]')\n",
    "\n",
    "    temp_values = []\n",
    "\n",
    "    for x in sim_matrix[food_det].values:\n",
    "        temp_values.append(x[0])\n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df[\"data\"] = temp_values\n",
    "    temp_df.index = sim_matrix.index\n",
    "\n",
    "    similar_food = list((temp_df.sort_values(by='data',ascending=True)).head(3).index)\n",
    "    recommend_df = food.loc[food['id'].isin(similar_food)]\n",
    "    recommend_df = recommend_df.drop(columns =[\"cluster_id\", 'count'])\n",
    "\n",
    "    print(recommend_df)\n",
    "\n",
    "    return(recommend_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "final_results = food_recommender(user_input)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from IPython.core.display import Image, display\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def rec_viz(final_results):\n",
    "    for row in final_results.values:\n",
    "        print(\"food_id: \" + str(row[0]))\n",
    "        query = str(row[1])\n",
    "    \n",
    "        for j in search(query, tld=\"co.in\", num=1, stop=1, pause=0):\n",
    "            url = j\n",
    "                \n",
    "            def getdata(url): \n",
    "                r = requests.get(url) \n",
    "                return r.text \n",
    "                \n",
    "            htmldata = getdata(url) \n",
    "            soup = BeautifulSoup(htmldata, 'html.parser')\n",
    "            title= str(soup.find_all('title'))\n",
    "            \n",
    "            title = title.replace('<title>','')\n",
    "            title = title.replace('</title>','')\n",
    "            title = title.replace(']','')\n",
    "            title = title.replace('[','')\n",
    "            title = title.replace('<title data-react-helmet=\"true\">', '')\n",
    "            title = title.split(\", \")\n",
    "            print(title[0])\n",
    "            \n",
    "            \n",
    "            counter = 0\n",
    "            for item in soup.find_all('img'):\n",
    "                if '//' in item['src']:\n",
    "                    \n",
    "                    if counter <= 0:\n",
    "                        #print(item['src'])\n",
    "                        display(Image(url= item['src'], width = 300, height = 300, unconfined=True))\n",
    "\n",
    "                        #display(Image(url= item['src'], unconfined=True))\n",
    "                    counter+=1\n",
    "                    \n",
    "rec_viz(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92461b6067541bcf763ace4906c527d223912732a8decd553ce7678dba7edd63"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
